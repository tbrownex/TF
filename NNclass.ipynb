{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadParms(featureCount):\n",
    "    parms = {}\n",
    "    parms[\"featureCount\"]  = featureCount\n",
    "    parms[\"l1Size\"]     = 6\n",
    "    parms[\"l2Size\"]     = 3\n",
    "    parms[\"l3Size\"]     = 2\n",
    "    parms[\"l4Size\"]     = parms[\"l2Size\"]\n",
    "    parms[\"l5Size\"]     = parms[\"l1Size\"]\n",
    "    parms[\"lr\"]         = .0002\n",
    "    #parms[\"Lambda\"]     = 10e-5          # Regularization parameter\n",
    "    parms[\"batchSize\"]  = 50\n",
    "    parms[\"epochs\"]     = 10\n",
    "    parms[\"activation\"] = tf.nn.tanh           # 'tanh' 'leakyReLU' 'ReLU' 'relu6' 'elu' 'crelu'\n",
    "    parms[\"stdDev\"]     = [0.2]\n",
    "    return parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    folder = \"/home/tbrownex/data/test/\"\n",
    "    file   = \"autoEncoder.csv\"\n",
    "    df = pd.read_csv(folder+file,index_col=0)\n",
    "    cols = [\"v1\",\"v2\",\"v3\",\"v4\",\"v5\",\"v6\",\"v7\",\"v8\",\"v9\",\"label\"]\n",
    "    df.columns = cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(df):\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train = train[train['label'] == 0]     # Train on \"normal\" data only\n",
    "    del train[\"label\"]\n",
    "    del test[\"label\"]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    test  = scaler.transform(test)\n",
    "    train, val = train_test_split(train, test_size=0.1)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, parms):\n",
    "        self._cost = None\n",
    "        self._train = None\n",
    "        \n",
    "        self.X  = tf.placeholder(\"float\", shape=[None, featureCount], name=\"input\")\n",
    "\n",
    "        self.l1w = tf.Variable(tf.truncated_normal([parms[\"featureCount\"], parms[\"l1Size\"]], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "        self.l2w = tf.Variable(tf.truncated_normal([parms[\"l1Size\"], parms[\"l2Size\"]], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "        self.l3w = tf.Variable(tf.truncated_normal([parms[\"l2Size\"], parms[\"l3Size\"]], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "        self.l4w = tf.Variable(tf.truncated_normal([parms[\"l3Size\"], parms[\"l4Size\"]], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "        self.l5w = tf.Variable(tf.truncated_normal([parms[\"l4Size\"], parms[\"l5Size\"]], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "        self.l6w = tf.Variable(tf.truncated_normal([parms[\"l5Size\"], featureCount], stddev=parms[\"stdDev\"], dtype=tf.float32))\n",
    "\n",
    "        self.l1b = tf.Variable(tf.zeros(parms[\"l1Size\"]))\n",
    "        self.l2b = tf.Variable(tf.zeros(parms[\"l2Size\"]))\n",
    "        self.l3b = tf.Variable(tf.zeros(parms[\"l3Size\"]))\n",
    "        self.l4b = tf.Variable(tf.zeros(parms[\"l4Size\"]))\n",
    "        self.l5b = tf.Variable(tf.zeros(parms[\"l5Size\"]))\n",
    "        self.l6b = tf.Variable(tf.zeros(featureCount))\n",
    "        \n",
    "        actf = parms[\"activation\"]\n",
    "        self.lr = parms[\"lr\"]\n",
    "        \n",
    "        l1Out = actf(tf.matmul(self.X,self.l1w)+self.l1b)\n",
    "        l2Out = actf(tf.matmul(l1Out,self.l2w)+self.l2b)\n",
    "        l3Out = actf(tf.matmul(l2Out,self.l3w)+self.l3b)\n",
    "        l4Out = actf(tf.matmul(l3Out,self.l4w)+self.l4b)\n",
    "        l5Out = actf(tf.matmul(l4Out,self.l5w)+self.l5b)\n",
    "        self.output = actf(tf.matmul(l5Out,self.l6w)+self.l6b)\n",
    "    \n",
    "    @property\n",
    "    def cost(self):\n",
    "        if self._cost is None:\n",
    "            self._cost = tf.reduce_mean(tf.square(self.output - self.X))\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self._train is None:\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self._train = optimizer.minimize(self.cost)\n",
    "        return self._train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbrownex/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "df = getData()\n",
    "trainData, valData, testData = prepData(df)\n",
    "featureCount = trainData.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = loadParms(featureCount)\n",
    "m = Model(parms)\n",
    "training = m.train\n",
    "valCost  = m.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 training batches of size 50\n"
     ]
    }
   ],
   "source": [
    "batchSize = 50\n",
    "epochs    = 50\n",
    "num_training_batches = int(trainData.shape[0] / batchSize)\n",
    "print(\"{} training batches of size {}\".format(num_training_batches, batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error (mse): 0.027\n"
     ]
    }
   ],
   "source": [
    "bestCost = 99.\n",
    "earlyStopping = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        data = shuffle(trainData)\n",
    "        for j in range(num_training_batches):\n",
    "            x_mini = trainData[j*batchSize:j*batchSize+batchSize]\n",
    "            _ = sess.run(training, feed_dict = {m.X: x_mini})\n",
    "        if i%5 ==0:\n",
    "            valLoss = sess.run(valCost, feed_dict={m.X:valData})\n",
    "            if valLoss/bestCost > 0.003:\n",
    "                bestCost = valLoss\n",
    "                earlyStopping = 0\n",
    "            else:\n",
    "                earlyStopping += 1\n",
    "        if earlyStopping > 3:    # no improvement for a while so stop\n",
    "            break\n",
    "    testLoss, recreation = sess.run([valCost, m.output], feed_dict={m.X:testData})\n",
    "    print(\"{}{:.3f}\".format(\"Reconstruction error (mse): \", testLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
